











import cnn
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
import torch
import torch.nn.functional as F
import trainer
from torch import nn
from torchvision import datasets
from torchvision.transforms import ToTensor





train_data = datasets.MNIST(
    root="data",
    train=True,
    transform=ToTensor(),
    download=True,
)
test_data = datasets.MNIST(root="data", train=False, transform=ToTensor())

batch_size = 100
train_loader = torch.utils.data.DataLoader(
    train_data, batch_size=batch_size, shuffle=True
)
test_loader = torch.utils.data.DataLoader(
    test_data, batch_size=batch_size, shuffle=False
)





if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Device set to mps")
else:
    device = torch.device("cpu")
    print("Device set to cpu")





X_np = np.expand_dims(train_data.data.float().clone().detach().numpy(), 1) / 255.0
y_np = train_data.targets.clone().detach().numpy()

test_X_np = np.expand_dims(test_data.data.float().clone().detach().numpy(), 1) / 255.0
test_y_np = test_data.targets.clone().detach().numpy()








loss_fn = nn.CrossEntropyLoss()
test_loss_fn = nn.CrossEntropyLoss(reduction="sum")





epochs = 15
lr = 0.001  # learning rate
test_period = 100








class ConvNetSimple(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 2, 5, padding=0)
        self.fc1 = nn.Linear(2 * 24 * 24, 200)
        self.fc2 = nn.Linear(200, 10)

    def forward(self, x):
        x = F.relu(self.conv(x))
        x = torch.flatten(x, 1)  # vectorize
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x





model_simple = ConvNetSimple().to(device)
optimizer_simple = torch.optim.Adam(model_simple.parameters(), lr=lr)
cnn_trainer_simple = trainer.Trainer(
    model_simple,
    device,
    optimizer_simple,
    loss_fn,
    test_loss_fn,
    train_loader,
    test_loader,
    epochs,
    test_period,
)





get_ipython().run_cell_magic("timeit", " -n 1 -r 1", """cnn_trainer_simple.train()""")





layer_simple0 = cnn.ConvLayer(channels_in=1, channels_out=2, dim_in=28,
                              dim_out=24, dim_W=5, stride=1, eta = lr)
layer_simple1 = cnn.VeccingLayer(channels_in=2, dim_in=24)
layer_simple2 = cnn.DenseLayer(dim_in=2 * 24 * 24, dim_out=200, eta = lr)
layer_simple3 = cnn.SoftmaxLayer(dim_in=200, dim_out=10, eta = lr)
my_cnn_simple = cnn.Neural(epochs=epochs, num_classes=10)
my_cnn_simple.add_layer(layer_simple0)
my_cnn_simple.add_layer(layer_simple1)
my_cnn_simple.add_layer(layer_simple2)
my_cnn_simple.add_layer(layer_simple3)





get_ipython().run_cell_magic("timeit", " -n 1 -r 1", """my_cnn_simple.fit(
    X_np,
    y_np,
    batch_size=batch_size,
    test_X=test_X_np,
    test_y=test_y_np,
    test_period=test_period,
)""")








def moving_average(array, w=3):
    psum = np.cumsum(array, dtype=float)
    psum[w:] = psum[w:] - psum[:-w]
    return psum[w - 1 :] / w





fig, ax = plt.subplots()

ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

w = 20 # iteration window size of moving average
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer_simple.losses) - w + 1),
    moving_average(cnn_trainer_simple.losses, w),
    label="PyTorch: training",
)
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer_simple.test_losses)),
    cnn_trainer_simple.test_losses,
    label="PyTorch: test",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.costs) - w + 1),
    moving_average(my_cnn_simple.costs, w),
    label="My CNN: training",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.test_costs)),
    my_cnn_simple.test_costs,
    label="My CNN: test",
)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.ylim(0, 0.2)
plt.grid()
plt.legend()
plt.show()





fig, ax = plt.subplots()

ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

w = 20  # iteration window size of moving average
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer_simple.errors) - w + 1),
    moving_average(cnn_trainer_simple.errors, w),
    label="PyTorch: training",
)
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer_simple.test_errors)),
    cnn_trainer_simple.test_errors,
    label="PyTorch: test",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.errors) - w + 1),
    moving_average(my_cnn_simple.errors, w),
    label="My CNN: training",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.test_errors)),
    my_cnn_simple.test_errors,
    label="My CNN: test",
)
plt.xlabel("Epoch")
plt.ylabel("Error")
plt.ylim(0, 0.05)
plt.grid()
plt.legend()
plt.show()








class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 2, 5, padding=2)
        self.conv2 = nn.Conv2d(2, 4, 5, stride=2)
        self.conv3 = nn.Conv2d(4, 8, 4, stride=2, padding=1)
        self.fc1 = nn.Linear(8 * 7 * 7, 200)
        # self.conv3 = nn.Conv2d(4, 8, 4, stride=2, padding=3)
        # self.fc1 = nn.Linear(8 * 9 * 9, 200)
        self.fc2 = nn.Linear(200, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        # the architecture requires uneven padding on left to
        # right and top to bottom of image.
        x = F.pad(x, (2, 1, 2, 1))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        # flatten all dimensions except batch
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


model = ConvNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
cnn_trainer = trainer.Trainer(
    model,
    device,
    optimizer,
    loss_fn,
    test_loss_fn,
    train_loader,
    test_loader,
    epochs,
    test_period,
)


get_ipython().run_cell_magic("timeit", " -n 1 -r 1", """cnn_trainer.train()""")





layer0 = cnn.ConvLayer(channels_in=1, channels_out=2, dim_in=28,
                       dim_out=28, dim_W=5, stride=1, eta=lr)
layer1 = cnn.ConvLayer(channels_in=2, channels_out=4, dim_in=28,
                       dim_out=14, dim_W=5, stride=2, eta=lr)
layer2 = cnn.ConvLayer(channels_in=4, channels_out=8, dim_in=14,
                       dim_out=7, dim_W=4, stride=2, eta=lr)
layer3 = cnn.VeccingLayer(channels_in=8, dim_in=7)
layer4 = cnn.DenseLayer(dim_in=8 * 7 * 7, dim_out=200, eta=lr)
layer5 = cnn.SoftmaxLayer(dim_in=200, dim_out=10, eta=lr)
my_cnn = cnn.Neural(epochs=epochs, num_classes=10)
my_cnn.add_layer(layer0)
my_cnn.add_layer(layer1)
my_cnn.add_layer(layer2)
my_cnn.add_layer(layer3)
my_cnn.add_layer(layer4)
my_cnn.add_layer(layer5)


epochs


get_ipython().run_cell_magic("timeit", " -n 1 -r 1", """my_cnn.fit(
    X_np, y_np, batch_size=100, test_X=test_X_np, test_y=test_y_np, test_period=100
)""")








fig, ax = plt.subplots()

ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

# iteration window size of moving average
w = 20
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer.losses) - w + 1),
    moving_average(cnn_trainer.losses, w),
    label="PyTorch: training",
)
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer.test_losses)),
    cnn_trainer.test_losses,
    label="PyTorch: validation",
)

plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.costs)-w+1),
    moving_average(my_cnn_simple.costs, w),
    label="My CNN: training",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.test_costs)),
    my_cnn_simple.test_costs,
    label="My CNN: validation",
)

plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.ylim(0, 0.2)
plt.grid()
plt.legend()
plt.show()
#plt.savefig("loss.pdf")





fig, ax = plt.subplots()

ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))

w = 20 # iteration window size of moving average
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer.errors) - w + 1),
    moving_average(cnn_trainer.errors, w),
    label="PyTorch: training",
)
plt.plot(
    np.linspace(0, epochs, len(cnn_trainer.test_errors)),
    cnn_trainer.test_errors,
    label="PyTorch: test",
)

plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.errors)-w+1),
    moving_average(my_cnn_simple.errors, w),
    label="My CNN: training",
)
plt.plot(
    np.linspace(0, epochs, len(my_cnn_simple.test_errors)),
    my_cnn_simple.test_errors,
    label="My CNN: test",
)

plt.xlabel("Epoch")
plt.ylabel("Error")
plt.ylim(0, 0.05)
plt.grid()
plt.legend()
plt.show()
#plt.savefig("error.pdf")



